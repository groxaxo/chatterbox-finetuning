from dataclasses import dataclass

@dataclass
class TrainConfig:
    # --- Paths ---
    # Directory where setup.py downloaded the files
    model_dir: str = "./pretrained_models"
    
    # Path to your metadata CSV (Format: ID|RawText|NormText)
    csv_path: str = "./MyTTSDataset/metadata.csv"
    metadata_path: str = "./metadata.json"
    
    # Directory containing WAV files
    wav_dir: str = "./MyTTSDataset/wavs"
    #wav_dir: str = "./FileBasedDataset"
    
    preprocessed_dir = "./MyTTSDataset/preprocess"
    #preprocessed_dir = "./FileBasedDataset/preprocess"
    
    # Output directory for the finetuned model
    output_dir: str = "./chatterbox_output"
    
    is_inference = False
    inference_prompt_path: str = "./speaker_reference/2.wav"
    inference_test_text: str = "Merhaba, sesimi geliştirmem oldukça uzun zaman aldı ve şimdi sahip olduğuma göre, sessiz kalmayacağım."


    ljspeech = True # Set True if the dataset format is ljspeech, and False if it's file-based.
    json_format = False # Set True if the dataset format is json, and False if it's file-based or ljspeech.
    preprocess = True # If you've already done preprocessing once, set it to false.
    
    is_turbo: bool = False # Set True if you're training Turbo, False if you're training Normal.

    # --- Vocabulary ---
    # The size of the NEW vocabulary (from tokenizer.json)
    # Ensure this matches the JSON file generated by your tokenizer script.
    # For Turbo mode: Use the exact number provided by setup.py (e.g., 52260)
    new_vocab_size: int = 2454  # Will be set in __post_init__ based on is_turbo

    # --- Hyperparameters ---
    batch_size: int = 4         # Adjust based on VRAM (2, 4, 8)
    grad_accum: int = 2        # Effective Batch Size = Batch * Accum
    learning_rate: float = 5e-5 # T3 is sensitive, keep low
    num_epochs: int = 250
    
    save_steps: int = 1000
    save_total_limit: int = 2
    dataloader_num_workers: int = 4

    # --- Constraints ---
    start_text_token = 255
    # For Turbo mode, use GPT-2 EOS token (50256); for Standard mode, use 0
    # Will be set in __post_init__ based on is_turbo
    stop_text_token: int = 0
    max_text_len: int = 256
    max_speech_len: int = 850   # Truncates very long audio
    prompt_duration: float = 3.0 # Duration for the reference prompt (seconds)
    
    def __post_init__(self):
        """Set mode-dependent values after initialization"""
        # Set vocab size based on mode
        if self.is_turbo and self.new_vocab_size == 2454:
            self.new_vocab_size = 52260
        
        # Set EOF token based on mode
        if self.is_turbo:
            self.stop_text_token = 50256  # GPT-2 EOS token
        else:
            self.stop_text_token = 0
